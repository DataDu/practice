{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math5671 Group3 Assignment 02\n",
    "\n",
    "**Hukai Luo, Tae Park, Xuanbo Huang**\n",
    "\n",
    "In this assignment, we will rewrite the sample code using TensorFlow based on the example. Try different values with the parameters for better results.\n",
    "\n",
    "## Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "Q = tf.Variable(tf.zeros([env.observation_space.n,env.action_space.n]),  dtype=tf.float32)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#discount factor\n",
    "gamma = tf.constant(0.8)\n",
    "\n",
    "#learning rate\n",
    "alpha = tf.constant(0.1)\n",
    "\n",
    "#total number of episodes you want to play during training\n",
    "episodes = 100\n",
    "total = tf.constant(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import several packages and set learning parameters\n",
    "## Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "<tf.Tensor 'cond/Merge:0' shape=() dtype=int64>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b2b1d77640a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[1;31m# step(self, action): Step the environment by one timestep. Returns observation, reward, done, info.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gym\\envs\\toy_text\\discrete.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: <tf.Tensor 'cond/Merge:0' shape=() dtype=int64>"
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = True\n",
    "    epsilon = tf.divide(1.0,1+i)\n",
    "    \n",
    "    while done:\n",
    "        \n",
    "        def f1(): return tf.convert_to_tensor(env.action_space.sample(), preferred_dtype=tf.int64)\n",
    "        def f2(): return tf.argmax(Q[state], output_type=tf.int64)\n",
    "        action = tf.cond(epsilon < tf.random_uniform((),0,1),true_fn = f1,false_fn = f2)\n",
    "        new_state, reward, done, info = env.step(int(sess.run(action)))\n",
    "    \n",
    "    \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # step(self, action): Step the environment by one timestep. Returns observation, reward, done, info.\n",
    "        \n",
    "        # Update Q-Table\n",
    "        # Q[state,action] = (1-alpha)*Q[state,action] + alpha*(reward + gamma * np.max(Q[new_state]))\n",
    "        X = tf.multiply(gamma,tf.reduce_max(Q[new_state]))  \n",
    "        Y = tf.add(X,reward)\n",
    "        Z = tf.subtract(Y,Q[state,action])\n",
    "        W = tf.multiply(alpha,Z)\n",
    "        V = tf.add(Q[state, action], W)\n",
    "        \n",
    "        #update the new value to q-table\n",
    "        update = tf.assign(Q[state, action],V)\n",
    "        sess.run(update)\n",
    "        state = new_state\n",
    "        total = tf.add(total,reward)\n",
    "        \n",
    "print(\"Average rewards: \"+str(sess.run(tf.divide(total,episodes))))\n",
    "print(\"Learned Q-Table\")\n",
    "print(sess.run(Q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "Q = tf.Variable(tf.random_uniform([env.observation_space.n,env.action_space.n],0,0),  dtype=tf.float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#discount factor\n",
    "gamma = tf.constant(0.8)\n",
    "\n",
    "#learning rate\n",
    "alpha = tf.constant(0.1)\n",
    "\n",
    "#total number of episodes you want to play during training\n",
    "episodes = 1000\n",
    "\n",
    "total = tf.constant(0, dtype=tf.float32)\n",
    "i = 0\n",
    "\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = True\n",
    "    epsilon = tf.divide(1.0,1+i)\n",
    "    \n",
    "    while done:\n",
    "        \n",
    "        def f1(): return tf.convert_to_tensor(env.action_space.sample(), preferred_dtype=tf.int64)\n",
    "        def f2(): return tf.argmax(Q[state], output_type=tf.int64)\n",
    "        action = tf.cond(epsilon < tf.random_uniform((),0,1),true_fn = f1,false_fn = f2)\n",
    "        new_state, reward, done, info = env.step(int(sess.run(action)))\n",
    "\n",
    "        # Update Q-Table\n",
    "        # Q[state,action] = (1-alpha)*Q[state,action] + alpha*(reward + gamma * np.max(Q[new_state]))\n",
    "        X = tf.multiply(gamma,tf.reduce_max(Q[new_state]))  \n",
    "        Y = tf.add(X,reward)\n",
    "        Z = tf.subtract(Y,Q[state,action])\n",
    "        W = tf.multiply(alpha,Z)\n",
    "        V = tf.add(Q[state, action], W)\n",
    "        \n",
    "        #update the new value to q-table\n",
    "        update = tf.assign(Q[state, action],V)\n",
    "        sess.run(update)\n",
    "        state = new_state\n",
    "        total = tf.add(total,reward)\n",
    "        \n",
    "print(\"Average rewards: \" + str(sess.run(tf.divide(total,episodes))))\n",
    "print(\"Learned Q-Table\")\n",
    "print(sess.run(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the Q-table useful and efficient, the parameter 'episodes' should be large enough(more than 5000). However, the program running slowly, it may take several hours to run it. This program will give poor performance because it creates several new TensorFlow graph nodes per operation. The underlying assumption in TensorFlow is (approximately) that we'll build a graph once and then call sess.run() on (various parts of) it multiple times. The first time we run a graph is relatively expensive, because TensorFlow has to build various data structures and optimize the execution of the graph across multiple devices.\n",
    "\n",
    "$\\alpha$ is the learning rate, $\\gamma$ is the discount factor that controls the importance of future rewards. $Q(s_t,a_t)$ is the Q-table value of action a t in state s t and time t. In the assignment, we try defferent values with parameters for better results. We set $\\alpha=0.5,\\gamma=0.1,episodes=10000$, get average reward=0.0147; we also set $\\alpha=0.9,\\gamma=0.1,episodes=10000$, get average reward=0.0161; set $\\alpha=0.6,\\gamma=0.1,episodes=10000$, get average reward=0.0163;set $\\alpha=0.8,\\gamma=0.5,episodes=10000$, get average reward=0.0134.\n",
    "\n",
    "**If the program runs for a very long time, just set the episodes = 10, it will save a lot of time, but the Q-table results may not be very good.**\n",
    "# Part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************\n",
      "Episodes 9\n",
      "*****************\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "*****************\n",
      "Steps 14\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "env.reset()\n",
    "episodes = tf.constant(10)\n",
    "max_steps = tf.constant(1000)\n",
    "episode = 0\n",
    "\n",
    "for episode in range(sess.run(episodes)):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    clear_output()\n",
    "    print(\"*****************\")\n",
    "    print(\"Episodes\", episode)\n",
    "    print(\"*****************\")\n",
    "    \n",
    "    for step in range(sess.run(max_steps)):        \n",
    "        # Take the action that has the best Q value\n",
    "        action = int(sess.run(tf.argmax(Q[state])))      \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # Only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "            env.render()\n",
    "            print(\"*****************\")\n",
    "            print(\"Steps\", step)\n",
    "            print(\"*****************\")\n",
    "            time.sleep(2)\n",
    "            break\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will take the action that has the best Q value and only print the last state (to see if our agent is on the goal or fall into an hole)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
